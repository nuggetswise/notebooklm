Subject: An AI glossary
From: Lenny's Newsletter <lenny@substack.com>
Date: 2025-06-24T12:03:12+00:00
Label: AI
ID: 84aadeb3-da67-4160-a685-6e838ac72838
--------------------------------------------------------------------------------

View this post on the web at https://www.lennysnewsletter.com/p/an-ai-glossary

üëã Welcome to a¬†üîí subscriber-only edition üîí¬†of my weekly newsletter. Each week I tackle reader questions about building product, driving growth, and accelerating your career. For more: Lennybot [ https://substack.com/redirect/d705af86-13d8-4776-b953-05967509ff38?j=eyJ1IjoiYXpucmUifQ.-8uBcpk8-HDATmkL0LtRV3v-RBLbqZCsv5EbXeOlA6M ] | Podcast [ https://substack.com/redirect/5e0f7671-ccd8-44b9-8238-7aa9cb9d0074?j=eyJ1IjoiYXpucmUifQ.-8uBcpk8-HDATmkL0LtRV3v-RBLbqZCsv5EbXeOlA6M ] | Hire your next product leader [ https://substack.com/redirect/d13ff95a-912f-42f8-9426-18aa6b7502b9?j=eyJ1IjoiYXpucmUifQ.-8uBcpk8-HDATmkL0LtRV3v-RBLbqZCsv5EbXeOlA6M ] | My favorite Maven courses [ https://substack.com/redirect/de69d481-fbd7-4466-9e74-027b3c4fee8f?j=eyJ1IjoiYXpucmUifQ.-8uBcpk8-HDATmkL0LtRV3v-RBLbqZCsv5EbXeOlA6M ] | Swag [ https://substack.com/redirect/b07c5276-ef21-4439-931d-ab46bd71cfe9?j=eyJ1IjoiYXpucmUifQ.-8uBcpk8-HDATmkL0LtRV3v-RBLbqZCsv5EbXeOlA6M ]
You‚Äôre probably hearing a lot of AI jargon, and you probably sort of know what some of it means . . . but not really. Below is an ‚Äúexplain it to me like I‚Äôm 5‚Äù definition of the 20+ most common AI terms, drawn from my own understanding, a bunch of research, and feedback from my most AI-pilled friends.
If you already know all this, no sweat, this post isn‚Äôt for you. For everyone else, keep the following list handy next time you‚Äôre in a meeting and you‚Äôre struggling to keep up with all the AI words flying around the room. I‚Äôll continue adding to this list as new buzzwords emerge.
P.S. If you prefer, you can listen to this post in convenient podcast form: Spotify [ https://substack.com/redirect/92c11d5c-6bd5-47b3-a6bf-12e3ecce8830?j=eyJ1IjoiYXpucmUifQ.-8uBcpk8-HDATmkL0LtRV3v-RBLbqZCsv5EbXeOlA6M ] / Apple [ https://substack.com/redirect/859eccf3-18e6-4884-9513-dc61e708c30e?j=eyJ1IjoiYXpucmUifQ.-8uBcpk8-HDATmkL0LtRV3v-RBLbqZCsv5EbXeOlA6M ] / YouTube [ https://substack.com/redirect/7f2e81d8-0962-46cd-b781-1a0b9892fd31?j=eyJ1IjoiYXpucmUifQ.-8uBcpk8-HDATmkL0LtRV3v-RBLbqZCsv5EbXeOlA6M ].
Model
An AI model is a computer program that is built to work like a human brain. You give it some input (i.e. a prompt), it does some processing, and it generates a response.
Like a child, a model ‚Äúlearns‚Äù by being exposed to many examples of how people typically respond or behave in different situations. As it sees more and more examples, it begins to recognize patterns, understand language, and generate coherent responses.
There are many different types of AI models. Some, which focus on language‚Äîlike ChatGPT o3 [ https://substack.com/redirect/7172d39b-b8b7-4ecf-81cf-1bb9973b1bbf?j=eyJ1IjoiYXpucmUifQ.-8uBcpk8-HDATmkL0LtRV3v-RBLbqZCsv5EbXeOlA6M ], Claude Sonnet 4 [ https://substack.com/redirect/feeba9d3-9fe0-44f1-b0f2-bd50cb77ab0c?j=eyJ1IjoiYXpucmUifQ.-8uBcpk8-HDATmkL0LtRV3v-RBLbqZCsv5EbXeOlA6M ], Gemini 2.5 Pro [ https://substack.com/redirect/22ea33a8-4fb7-4f02-9c50-4c8a1b1bef28?j=eyJ1IjoiYXpucmUifQ.-8uBcpk8-HDATmkL0LtRV3v-RBLbqZCsv5EbXeOlA6M ], Meta Llama 4 [ https://substack.com/redirect/7bbad08f-75aa-4a77-b7e2-337e1253205d?j=eyJ1IjoiYXpucmUifQ.-8uBcpk8-HDATmkL0LtRV3v-RBLbqZCsv5EbXeOlA6M ], Grok 3 [ https://substack.com/redirect/b2c853ca-622b-4dc8-8ab1-a98928470292?j=eyJ1IjoiYXpucmUifQ.-8uBcpk8-HDATmkL0LtRV3v-RBLbqZCsv5EbXeOlA6M ], DeepSeek [ https://substack.com/redirect/a689d8fd-6f0e-44f1-985d-e8a2298623be?j=eyJ1IjoiYXpucmUifQ.-8uBcpk8-HDATmkL0LtRV3v-RBLbqZCsv5EbXeOlA6M ], and Mistral [ https://substack.com/redirect/bff04f8e-8376-402a-be9d-7d48b75f3452?j=eyJ1IjoiYXpucmUifQ.-8uBcpk8-HDATmkL0LtRV3v-RBLbqZCsv5EbXeOlA6M ]‚Äîare known as large language models (LLMs). Others are built for video, like Google Veo 3 [ https://substack.com/redirect/129f5ce5-92da-49e1-b7dd-c98dbd91ac47?j=eyJ1IjoiYXpucmUifQ.-8uBcpk8-HDATmkL0LtRV3v-RBLbqZCsv5EbXeOlA6M ], OpenAI Sora [ https://substack.com/redirect/58b28897-f90c-4f53-919a-232d64fc0961?j=eyJ1IjoiYXpucmUifQ.-8uBcpk8-HDATmkL0LtRV3v-RBLbqZCsv5EbXeOlA6M ], and Runway Gen-4 [ https://substack.com/redirect/50b4ac82-aacb-42d0-93f5-fa40d59c2c88?j=eyJ1IjoiYXpucmUifQ.-8uBcpk8-HDATmkL0LtRV3v-RBLbqZCsv5EbXeOlA6M ]. Some models specialize in generating voice, such as ElevenLabs [ https://substack.com/redirect/881f7be9-66d3-4247-b699-9bbf748a517d?j=eyJ1IjoiYXpucmUifQ.-8uBcpk8-HDATmkL0LtRV3v-RBLbqZCsv5EbXeOlA6M ], Cartesia [ https://substack.com/redirect/b98f55cf-275e-4002-a243-ea6388111462?j=eyJ1IjoiYXpucmUifQ.-8uBcpk8-HDATmkL0LtRV3v-RBLbqZCsv5EbXeOlA6M ], and Suno [ https://substack.com/redirect/ba91c966-63c2-4993-ba57-c96b377f0c55?j=eyJ1IjoiYXpucmUifQ.-8uBcpk8-HDATmkL0LtRV3v-RBLbqZCsv5EbXeOlA6M ]. There are also more traditional types of AI models, such as classification models (used in tasks like fraud detection), ranking models (used in search engines, social media feeds, and ads), and regression models (used to make numerical predictions).
LLM (large language model)
LLMs are text-based models, designed to understand and generate human-readable text. That‚Äôs why the name includes the word ‚Äúlanguage.‚Äù 
Recently, most LLMs have actually evolved into ‚Äúmulti-modal‚Äù models that can process and generate not just text but also images, audio, and other types of content within a single conversational interface. For example, all of the ChatGPT LLM models natively support text, images, and even voice. This started with GPT-4o, where ‚Äúo‚Äù stands for ‚Äúomni‚Äù (meaning it accepts any combination of text, audio, and image input).
Here‚Äôs a really good primer on how LLMs actually work [ https://substack.com/redirect/b90c4224-838f-43ad-bc8b-c36896280d5b?j=eyJ1IjoiYXpucmUifQ.-8uBcpk8-HDATmkL0LtRV3v-RBLbqZCsv5EbXeOlA6M ], and also this popular deep dive by Andrej Karpathy [ https://substack.com/redirect/b4c8ce80-c53e-40fc-bc94-81ba048860b2?j=eyJ1IjoiYXpucmUifQ.-8uBcpk8-HDATmkL0LtRV3v-RBLbqZCsv5EbXeOlA6M ]:
Transformer
The transformer architecture, developed by Google researchers in 2017, is the algorithmic discovery that made modern AI (and LLMs in particular) possible.
Transformers introduced a mechanism called ‚Äúattention,‚Äù where instead of only being able to read text word‚Äëby‚Äëword, sequentially, the model is able to look at all the words at once. This helps the models understand how words relate to each other, making them far better at capturing meaning, context, and nuance than earlier techniques.
Another big advantage of the transformer architecture is that it‚Äôs highly parallelizable‚Äîit can process many parts of a sequence at the same time. This makes it possible to train much bigger and smarter models simply by scaling up the data and compute power. This breakthrough is why we suddenly went from basic chatbots to sophisticated AI assistants. Almost every major AI model today, including ChatGPT and Claude, is built on top of the transformer architecture.
This is the best explanation of transformers [ https://substack.com/redirect/1f7b0890-a9c3-475b-bde9-b90310a10472?j=eyJ1IjoiYXpucmUifQ.-8uBcpk8-HDATmkL0LtRV3v-RBLbqZCsv5EbXeOlA6M ] I‚Äôve seen. Here‚Äôs also a more technical and visual deep dive:
Training/Pre-training
Training is the process by which an AI model learns by analyzing massive amounts of data. This data might include large portions of the internet, every book ever published, audio recordings, movies, video games, etc. Training state-of-the-art models can take weeks or months, require processing terabytes of data, and cost hundreds of millions of dollars.
For LLMs, the core training method is called ‚Äúnext-word prediction.‚Äù The model is shown billions of text sequences with the last word hidden, and it learns to predict what word should come next. 
As it trains, the model adjusts millions of internal settings called ‚Äúweights.‚Äù These are similar to how neurons in the human brain strengthen or weaken their connections based on experience. When the model makes a correct prediction, those weights are reinforced. When it makes an incorrect one, they‚Äôre adjusted. Over time, this process helps the model improve its understanding of facts, grammar, reasoning, and how language works in different contexts. Here‚Äôs a quick visual explanation [ https://substack.com/redirect/06b78f30-4e5e-4f6b-a53e-2c4bab1f168d?j=eyJ1IjoiYXpucmUifQ.-8uBcpk8-HDATmkL0LtRV3v-RBLbqZCsv5EbXeOlA6M ].
If you‚Äôre skeptical of next-word prediction generating novel insights and super-intelligent AI systems, here‚Äôs Ilya Sutskever (co-founder of OpenAI) explaining why it‚Äôs deceptively powerful:
Supervised learning
Supervised learning refers to when a model is trained on ‚Äúlabeled‚Äù data‚Äîmeaning the correct answers are provided. For example, the model might be given thousands of emails labeled ‚Äúspam‚Äù or ‚Äúnot spam‚Äù and, from that, learn to spot the patterns that distinguish spam from non-spam. Once trained, the model can then classify new emails it‚Äôs never seen before.
Most modern language models, including ChatGPT, use a subtype called ‚Äúself-supervised learning.‚Äù Instead of relying on human-labeled data, the model creates its own labels, generally by hiding the last word of a sentence and learning to predict it. This allows it to learn from massive amounts of raw text without manual annotation.
Unsupervised learning
Unsupervised learning is the opposite: the model is given data without any labels or answers. Its job is to discover patterns or structure on its own, like grouping similar news articles together or detecting unusual patterns in a dataset. This method is often used for tasks like anomaly detection, clustering, and topic modeling, where the goal is to explore and organize information rather than make specific predictions.
Post-training
Post-training refers to all of the additional steps taken after training is complete to make the model even more useful. This includes steps like ‚Äúfine-tuning‚Äù and ‚ÄúRLHF.‚Äù
Fine-tuning
Fine-tuning is a post-training technique where you take a trained model and do additional training on specific data that‚Äôs tailored to what you want the model to be especially good at. For example, you would fine-tune a model on your company‚Äôs customer service conversations to make it respond in your brand‚Äôs specific style, or on medical literature to make it better at answering healthcare questions, or on educational content for specific grade levels to create a tutoring assistant that explains concepts in age-appropriate ways.
This additional training tweaks the model‚Äôs internal weights to specialize its responses for your specific use case, while preserving the general knowledge it learned during pre-training. 
Here‚Äôs an awesome technical deep dive into how fine-tuning works:
RLHF (reinforcement learning from human feedback)
RLHF is a post-training technique that goes beyond next-word prediction and fine-tuning by teaching AI models to behave the way humans want them to‚Äîmaking them safer, more helpful, and aligned with our intentions. RLHF is the key method used for what‚Äôs referred to as ‚Äúalignment.‚Äù
This process works in two stages: First, human evaluators compare pairs of outputs and choose which is better, training a ‚Äúreward model‚Äù that learns to predict human preferences. Then, the AI model learns through reinforcement learning‚Äîa trial-and-error process where it receives ‚Äúrewards‚Äù from the reward model (not directly from humans) for generating responses the reward model predicts humans would prefer. In this second stage, the model is essentially trying to ‚Äúgame‚Äù the reward model to get higher scores.
Here‚Äôs a great guide [ https://substack.com/redirect/c60b227f-0016-4fcf-90e9-66517b7ad1bb?j=eyJ1IjoiYXpucmUifQ.-8uBcpk8-HDATmkL0LtRV3v-RBLbqZCsv5EbXeOlA6M ], plus this technical deep dive into RLHF:
Prompt engineering
Prompt engineering is the art and science of crafting questions (i.e. ‚Äúprompts‚Äù) for AI models that result in better and more useful responses. Like when you‚Äôre talking to a person, the way you phrase your question can lead to dramatically different responses. The same AI model will give very different responses based on how you craft your prompt.
There are two categories of prompts:
Conversational prompts: What you send ChatGPT/Claude/Gemini when you‚Äôre having a conversation with it
System/product prompts: The behind-the-scenes instructions that developers bake into products to shape how the AI product behaves
Here‚Äôs a podcast episode from just last week where we cover this and much more:
RAG (retrieval-augmented generation)
RAG is a technique that gives models access to additional information at run-time that they weren‚Äôt trained on. It‚Äôs like giving the model an open-book test instead of having it answer from memory.
When you ask a question like ‚ÄúHow do this month‚Äôs sales compare to last month?‚Äù a retrieval system is able to search through your databases, documents, and knowledge repos to find pertinent information. This retrieved data is then added as context to your original prompt, creating an enriched prompt that the model then processes. This leads to a much better, more accurate answer.
If you don‚Äôt give the model the context it needs to answer your question through RAG, this is when ‚Äúhallucinations‚Äù happen (see more below).
Broadly, to summarize:
Pre-training: Teaches the model general knowledge (and language)
Fine-tuning: Specializes the model for specific tasks
RLHF: Aligns the model with human preferences
Prompt engineering: The skills of crafting better inputs to guide the model toward the most useful outputs
RAG: A technique that retrieves additional relevant information from external sources at run-time to give the model up-to-date or task-specific context it wasn‚Äôt trained on
Here‚Äôs a great overview of fine-tuning vs. RAG vs. prompt engineering:
Inference
Inference is when the model ‚Äúruns.‚Äù When you ask ChatGPT a question and it generates a response, that‚Äôs it doing inference.
MCP (model context protocol)
MCP is a recently released open standard that allows AI models to interact with external tools‚Äîlike your calendar, CRM, Slack, or codebase‚Äîeasily, reliably, and securely. Previously, developers had to write their own custom code for each new integration.
MCP also gives the AI the ability to take actions through these tools, for example, updating customer records in Salesforce, sending messages in Slack, scheduling meetings in your calendar, or even committing code to GitHub.
It‚Äôs still early in the definition of AI protocols, and there are other competing proposals, like A2A from Google and ACP from BeeAI/IBM.
Here‚Äôs a really nice in-depth explanation of MCP:...

Unsubscribe https://substack.com/redirect/2/eyJlIjoiaHR0cHM6Ly93d3cubGVubnlzbmV3c2xldHRlci5jb20vYWN0aW9uL2Rpc2FibGVfZW1haWw_dG9rZW49ZXlKMWMyVnlYMmxrSWpveE9EUTFPVGt4TkN3aWNHOXpkRjlwWkNJNk1UVXpNamsyTURBekxDSnBZWFFpT2pFM05UQTNOamMxT0RZc0ltVjRjQ0k2TVRjNE1qTXdNelU0Tml3aWFYTnpJam9pY0hWaUxURXdPRFExSWl3aWMzVmlJam9pWkdsellXSnNaVjlsYldGcGJDSjkudF9uOWwxRVRmZV9WVGVhekZSWUl4aThxUk54Z2pYZ01Hd25TUEctUlVkcyIsInAiOjE1MzI5NjAwMywicyI6MTA4NDUsImYiOnRydWUsInUiOjE4NDU5OTE0LCJpYXQiOjE3NTA3Njc1ODYsImV4cCI6MjA2NjM0MzU4NiwiaXNzIjoicHViLTAiLCJzdWIiOiJsaW5rLXJlZGlyZWN0In0.LJrb4aKIcXrmuh4WsKj-sfopMPpL0pnooJ4o26l7e2E?