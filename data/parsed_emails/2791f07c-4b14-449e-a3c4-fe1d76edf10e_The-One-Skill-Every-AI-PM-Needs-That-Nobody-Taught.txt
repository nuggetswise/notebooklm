Subject: The One Skill Every AI PM Needs (That Nobody Taught You)
From: Aakash Gupta from Product Growth <aakashgupta@substack.com>
Date: 2025-06-06T22:48:33+00:00
Label: AI
ID: 2791f07c-4b14-449e-a3c4-fe1d76edf10e
--------------------------------------------------------------------------------

View this post on the web at https://www.news.aakashg.com/p/ai-evals

Just like you can't be a PM without using analytics, you can't be a PM on AI products without evals.
Unlike traditional software, LLM pipelines do not produce deterministic outputs. 
A response may be factually accurate but inappropriate (i.e., the â€œvibes are offâ€). 
They may sound persuasive while conveying incorrect information.
The core challenge is:  How do we assess whether an LLM pipeline is performing adequately? And how do we diagnose where it is failing?
To give you the absolute expert POV on this, Iâ€™ve teamed up with Hamel Husain [ https://substack.com/redirect/b72fd11f-0746-4134-86f2-5c4f09a13139?j=eyJ1IjoiYXpucmUifQ.-8uBcpk8-HDATmkL0LtRV3v-RBLbqZCsv5EbXeOlA6M ], one of the most recognized names in AI Evals industry-wide.
I have been taking his AI Evals for Engineers & PMs course [ https://substack.com/redirect/70ae8b70-ff4a-484c-8bb4-f47027acefec?j=eyJ1IjoiYXpucmUifQ.-8uBcpk8-HDATmkL0LtRV3v-RBLbqZCsv5EbXeOlA6M ], and itâ€™s great. Use my code ag-product-growth [ https://substack.com/redirect/70ae8b70-ff4a-484c-8bb4-f47027acefec?j=eyJ1IjoiYXpucmUifQ.-8uBcpk8-HDATmkL0LtRV3v-RBLbqZCsv5EbXeOlA6M ] to get over $800 off.
Defining Evals
What is an LLM evaluation (eval)?
Itâ€™s the systematic measurement of LLM pipeline quality. A good evaluation produces results that can be easily and unambiguously interpreted. It goes beyond a single number. 
Evals can be operationalized in many different ways. 
Three common ways are:
Background Monitoring - passively, without interrupting the core workflow, these evals detect drift or degradation.
Guardrails - in the critical path of the pipeline, these evals run can block the output, force a retry, or result in the fall back to a safer alternative. 
To Improve a pipeline - these evals can label data for fine-tuning LLMs, select high-quality few-shot examples for prompts, or identify failure cases that motivate architectural changes.
What Could Go Wrong? The Three Gulfs
Letâ€™s take the example of an email processing AI product. 
The goal is to: extract the senderâ€™s name, summarize the key requests, and categorize the emails.
This might seem simple. But we actually encounter three gulfs [ https://substack.com/redirect/70f0cba0-ca7b-42fc-9a76-aa7f2b93afbb?j=eyJ1IjoiYXpucmUifQ.-8uBcpk8-HDATmkL0LtRV3v-RBLbqZCsv5EbXeOlA6M ], as the famous paper Who Validates the Validators? explains:
Gulf 1 - Comprehension (Developer â†’ Data)
Thousands of inputs might arrive daily, with diverse formats and varying levels of clarity.
At scale, it is difficult to know the characteristics of this input data distribution, detect errors within it, or identify unusual patterns.
Just as we cannot read every input, we also cannot manually inspect every output trace generated by the pipeline to grasp all the subtle ways it might succeed or fail across the input space. 
So the dual challenge is: how can we understand the important properties of our input data, and the spectrum of our pipelineâ€™s behaviors and failures on that data, without examining every single example?
Gulf 2 - Specification (Developer â†’ LLM Pipeline)
Our intentâ€”the task we want the LLM to performâ€”is often only loosely captured by the prompts you write.
For example, we might write: 
Extract the senderâ€™s name and summarize the key requests in this email
At first glance, this sounds specific. 
But important questions are left unanswered: 
How concise or detailed should the summary be?
Should the summary be a paragraph or a bulleted list? 
Should the summary include implicit requests, or only explicit ones? 
Should the sender be the display name, the full email address, or both? 
The LLM cannot infer these decisions unless we explicitly specify them.
Gulf 3 - Generalization (Data â†’ LLM Pipeline)
Imagine an email that mentions a public figure, like Elon Musk or Donald Trump, within the body text. 
The model might mistakenly extract these names as the sender, even though they are unrelated to the actual email metadata.
This is a generalization failure: the model applies the instructions incorrectly because it has not generalized properly across diverse data.
Even when prompts are clear and well-scoped, and even as models get better, this gulf will always exist to some degree, because no model will ever be perfectly accurate on all inputs. 
Why Evals are Challenging
Developing effective evaluations for LLM pipelines is hard.
There are at least 4 reasons for this:
Each application requires bridging the Three Gulfs anew. There are no universal evaluation recipes.
Requirements often emerge only after interacting with early outputs. We might initially expect summaries to be written in prose, but later realize that bulleted lists are easier for users to scan. Evaluation criteria must evolve alongside system development.
Appropriate metrics are rarely obvious at the outset. Unlike traditional software, where correctness is well-defined, LLM pipelines involve tradeoffs: factual accuracy, completeness, conciseness, style, and more.
There is no substitute for examining real outputs on real data. Generic benchmarks cannot capture the specific failure modes of our pipeline. Systematic evaluation requires careful, hands-on analysis of representative examples.
The Solution: The LLM Evaluation Lifecycle
Evaluation provides the systematic means to understand and address these challenges. This is done to through 3 steps:
Step 1 - Analyze
Inspect the pipelineâ€™s behavior on representative data to qualitatively identify failure modes. 
This critical first step illuminates why the pipeline might be struggling. Failures uncovered often point clearly to: 
Ambiguous instructions (Specification issues), or 
Inconsistent performance across inputs (Generalization issues)
Understanding their true frequency, impact, and root causes demands quantitative data, hence Step 2â€¦
Step 2 - Measure
Develop and deploy specific evaluators (evals) to quantitatively assess the failure modes.
This data is crucial for prioritizing which problems to fix first and for diagnosing the underlying causes of tricky generalization failures.
Step 3 - Improve
Make targeted interventions.
This includes direct fixes to prompts and instructions addressing Specification issues identified during Analyze.
It also involves data-driven efforts: such as,  engineering better examples, refining retrieval strategies, adjusting architectures, or fine-tuning models to enhance generalization.
Cycling through Analyze, Measure, and Improve (and then back to Analyze) uses structured evaluation to systematically navigate the complexities posed by the Three Gulfs, leading to more reliable and effective LLM applications.
We will cover each phase in greater depth in the upcoming sections.
Where We Go From Here
So, thatâ€™s your introduction. Now, letâ€™s go a layer deeperâ€¦ 
Eval Basics
ðŸ”’ Step 1: Analyze
ðŸ”’ Step 2: Measure 
ðŸ”’ Step 3: Improve
1. The Layer Deeper: Eval Basics
Before we dive further into the practical evaluation lifecycle, letâ€™s understand:
What LLMs can and can't do
How to communicate with them effectively
And the basic types of evaluation metrics out there
LLM Strengths and Weaknesses
We all have experienced LLMâ€™s strengths: they can produce fluent text and generalize to new tasks easily.
But, despite their strengths, LLMs face 4 key limitations that stem from their architecture, training objective, and probabilistic nature:
Algorithmic tasks. They can't reliably execute loops or recursion. A model trained on 3-digit addition often fails on 5-digit sums.
Reliability. Outputs are probabilistic, not deterministic. The same prompt can yield different results.
Prompt sensitivity. Small wording changes can dramatically alter outputs.
Factuality. LLMs optimize for statistical likelihood, not truth. They can confidently assert false information.
Given these strengths and weaknesses, how do we effectively interact with LLMs? The primary method is prompting.
Prompting Fundamentals
What seems obvious to us might be unclear to the LLM. 
Precision in our prompt is key.
A well-structured prompt contains 7 key components:
Component 1 - Role and Objective
Clearly define the persona or role the LLM should adopt and its overall goal. This helps set the stage for the desired behavior. 
Example:
You are an expert technical writer tasked with explaining complex AI concepts to a non-technical audience.
Component 2 - Instructions/Response Rules
Be specific and unambiguous
Use bullet points for multiple instructions
Define what NOT to do
Example:
- Summarize the following research paper abstract.
- The summary must be exactly three sentences long.
- Avoid using technical jargon above a high-school reading level.
- Do not include any personal opinions or interpretations.
Component 3 - Context 
The relevant background information, data, or text the LLM needs to perform the task.
Component 4 - Examples (Few-Shot Prompting) 
One or more examples of desired input-output pairs. Highly effective for guiding format, style, and detail level.
Component 5 - Reasoning Steps (Chain-of-Thought) 
For complex problems, instruct the model to "think step by step" or outline a specific reasoning process.
Component 6 - Output Formatting Constraints
Explicitly define the desired structure, format, or constraints for the LLMâ€™s response. This is critical for programmatic use of the output.
Example:
Respond using only JSON format with the following keys: sender_name (string), main_issue (string), and suggested_action_items (array of strings).
Component 7 - Delimiters and Structure 
Use clear separators (### Instructions ###, triple backticks, XML tags) to distinguish different prompt components.
Precision in prompting is key to bridging the Gulf of Specification. 
However, finding the perfect prompt is rarely immediate. 
Itâ€™s an iterative process. 
Weâ€™ll write a prompt, test it on various inputs, analyze the outputs (using evaluation techniques weâ€™ll discuss in future sections), identify failure modes, and refine the prompt accordingly.
Note on Outsourcing Prompting
There are many tools that will write prompts for you and optimize them. 
Itâ€™s important that you avoid these in the beginning stages of development, as writing the prompt forces you to externalize your specification and clarify your thinking. 
People who delegate prompt writing to a black box too aggressively struggle to fully understand their failure modes. 
After you have some reps with looking at your data, you can introduce these tools (but do so carefully).
An iterative refinement process hinges on having clear ways to judge whether the output is good or bad. This brings us to the concept of evaluation metrics.
Types of Evaluation Metrics
Evaluation metrics provide systematic measurements of LLM pipeline quality. They fall into two categories [ https://substack.com/redirect/b294757c-01fd-4298-a945-e5d67e74ee2d?j=eyJ1IjoiYXpucmUifQ.-8uBcpk8-HDATmkL0LtRV3v-RBLbqZCsv5EbXeOlA6M ]:
Reference-Based Metrics
These evals compare the LLMâ€™s output against a known, ground-truth answer.
This is much like having an official answer key to grade a multiple-choice test. 
Reference-based metrics are often valuable during the development cycle, e.g., as unit tests. Think:
Keyword presence verification
Exact string matching for short-answer extraction
Beyond simple string comparisons, reference-based checks can become more complex. 
Sometimes, this involves executing the result and comparing the result with a reference output. For example:
Running generated code against unit tests
Executing generated SQL and checking if the output result or table matches a known correct result or table.
Reference-Free Metrics
Alternatively, reference-free metrics evaluate an LLMâ€™s output based on its inherent properties or whether it follows certain rules, operating without a specific â€œgoldenâ€ answer.
This approach becomes crucial when dealing with output that is subjective, creative, or where multiple valid responses can exist.
For example:
Verifying a chatbot doesn't offer medical advice
Ensuring generated code includes explanatory comments
Validating that generated SQL executes without syntax errors
Checking if a summary introduces speculation not in the source
These examples illustrate how reference-free qualities are highly application-specific.
Reference-free checks focus on validity rather than comparing results.
The distinction matters
Reference-based metrics are preferred when feasible because they're much cheaper to maintain and verify. 
Simple assertions and reference-based checks require minimal overhead, while LLM-as-Judge evaluators need 100+ labeled examples and ongoing maintenance.
Focus automated evaluators on failures that persist after fixing your prompts. 
Many teams build complex evaluation infrastructure for preferences they never actually specified, like wanting short responses or specific formatting. Fix these obvious specification gaps first.
Consider the cost hierarchy: 
Start with cheap code-based checks (regex, structural validation, execution tests)
Reserve expensive LLM-as-Judge evaluators for persistent generalization failures that can't be captured by simple rules.
The way we evaluate depends heavily on what we are evaluatingâ€”and the cost of maintaining that evaluation over time.
Foundation Models vs Application-Centric Evals
Foundation Model Evals (like MMLU, HELM, GSM8k) assess general capabilities and knowledge of base LLMs. Think of them as "standardized tests"â€”useful for initial model selection but insufficient for your specific application.
Application Evals assess whether your specific pipeline performs successfully on your specific task using your realistic data. These are your primary focus because:
Foundation models undergo opaque alignment processes
General post-training may not match your application's requirements
You need metrics capturing your specific quality criteria
Be extremely skeptical of generic metrics for your application. 
In most cases, they're a distraction. 
The quickest smell that an evaluation has gone off the rails is seeing a dashboard packed with generic metrics like "hallucination_score," "helpfulness_score," "conciseness_score."
Since application-specific evaluations often involve criteria beyond simple right/wrong answers, we need methods to systematically capture these judgments. How do we actually generate these evaluation signals?
Eliciting Labels for Metric Computation
How do we actually generate the scores or labels needed to compute our metrics?
This question is especially pertinent for reference-free metrics, where there is no golden answer key. How can we systematically judge qualities defined by our application, like "helpfulness" or "appropriateness"?
We recommend starting with binary Pass/Fail evaluations in most cases, as we'll explore in detail later. Binary evaluations force clearer thinking and more consistent labeling, while Likert scales introduce significant challenges: the difference between adjacent points (like 3 vs 4) is subjective and inconsistent across annotators, and annotators often default to middle values to avoid making hard decisions.
With this preference for binary judgments in mind, three general approaches are used:
Approach 1 - Direct Grading or Scoring
The most common method of evaluations for AI applications is Direct Grading or Scoring. Here, an evaluator assesses a single output against a predefined rubric.
This rubric might use a scale (e.g., 1-5 helpfulness) or categorical labels (e.g., Pass/Fail, Tone: Formal/Informal/Casual).
Evaluators can be human annotators, domain experts, or a well-prompted "LLM-as-judge," or an LLM that has been prompted to assess outputs according to the rubric.
Obtaining reliable direct grades demands extremely clear, unambiguous definitions for every possible score or label. Defining distinct, objective criteria for each point on a 1-5 scale can be surprisingly difficult.
For this reason, simpler binary judgments (like Yes/No for "Is this summary faithful to the source?") are often easier to define consistently and are our recommended starting point. 
Binary decisions are also faster to make during error analysisâ€”you don't waste time debating whether something is a 3 or 4.
Direct grading is most useful when our primary goal is assessing the absolute quality of a single pipeline's output against our specific, predefined standards.
Approach 2 - Pairwise Comparison
Pairwise comparison presents an evaluator with two outputs (A and B) generated for the same input prompt. The evaluator must then choose which output is better based on a specific, clearly defined criterion or rubric.
While this still requires unambiguous comparison criteria, making this relative choice between two options is frequently cognitively easier for evaluators than assigning a precise score from a multi-level scale.
Approach 3 - Ranking
Evaluators order three or more outputs generated for the same input from best to worst, according to the same clearly defined quality dimension specified in the comparison rubric.
Ranking provides more granular relative information than pairwise comparison, though it typically requires more evaluator effort.
These relative judgment methods are particularly useful when our main goal is to compare different systems or outputs directly. We might use them for A/B testing different prompts, comparing candidate models, or selecting the best response when our pipeline generates multiple options for a single input.
For tracking gradual improvements, consider measuring specific sub-components with their own binary checks rather than using scales. 
For example, instead of rating factual accuracy 1-5, track "4 out of 5 expected facts included" as separate binary checks. This preserves the ability to measure progress while maintaining clear, objective criteria.
The Layer Deeper - Step 1: Analyze
The process of developing robust evaluations for LLM applications is inherently iterative.
This section provides a detailed methodology for the Analyze portionâ€”specifically focusing on how we systematically surface failure modes.
Letâ€™s ground our discussion in a running example: a real estate CRM assistant.
The assistant powers real estate agents' workflows. Given natural language queries, it can generate SQL queries to retrieve listing data, summarize trends, draft emails to clients, and read calendars.
Typical user queries might include: "Find me 3-bedroom homes under $600k near downtown. Email the top 2 matches to my client. Figure out if there are showings available for this weekend."
The pipeline is agentic. An LLM call interprets the user's request and returns structured actions. Each action invokes a downstream tool.
Outputs are fed back to the LLM, which may issue further actions based on new information.
1. Bootstrap a Starting Dataset
Every error analysis starts with traces: the full sequence of inputs, outputs, and actions taken by the pipeline for a given input.
Ideally, we want to start with around 100 traces.
This gives enough coverage to surface a wide range of failure modes and push toward theoretical saturationâ€”the point at which analyzing additional traces is unlikely to reveal sufficiently new categories or types of errors.
If 100 real user queries already exist, we sample them directly.
Diversity matters: traces should stress different parts of the system, not just repeat the same feature path.
Systematic Synthetic Data Generation
In early-stage pipelines, real user traces are often sparse.
In those cases, we generate synthetic dataâ€”but carefully. We should not simply prompt an LLM to "give us user queries."
Naively generated queries tend to be generic, repetitive, and fail to capture real usage patterns.
For many applications, the variety of fundamental user intents is often surprisingly limitedâ€”perhaps on the order of 10 to 20 core types.
First, we define key dimensions of the query space.
Choose the dimensions that describe where your AI application is likely to fail.
Dimension
A dimension is a way to categorize different parts of a user query. Each dimension represents one axis of variation.
For example, in a real estate assistant, useful dimensions might include:
Feature: what task the user wants to perform (e.g., property search, scheduling)
Client Persona: the type of client being served (e.g., first-time buyer, investor)
Scenario Type: how clearly the user expresses their intent (e.g., well-specified, ambiguous)
Do not choose these dimensions arbitrarily! Instead, choose the dimensions that describe where your AI application is likely to fail.
Once we've defined our dimensions, we create structured combinations called tuples.
Tuple
A tuple is a specific combination of valuesâ€”one from each dimensionâ€”that defines a particular use case.
For example:
Feature: Property Search
Client Persona: Investor
Scenario Type: Ambiguous
This tuple describes a case where an investor is searching for properties, but the request may be underspecified or vague.
Here are two example queries sketched from different tuples:
Tuple: (Feature: 'Property Search', Persona: 'First Time Buyer', Scenario: 'Specific Query') Query: Find 3-bedroom homes under $600k near downtown that allow pets.
Tuple: (Feature: 'Property Search', Persona: 'Investor', Scenario: 'Vague Query') Query: Look up showings for good properties in San Mateo County.
Once we have a small set of example tuples and queries, we use an LLM to scale up to 100 or more.
But instead of having the LLM generate full queries directlyâ€”which often leads to repetitive phrasingâ€”we break the process into two steps.
First, we sample structured tuples: feature, persona, and scenario. Then, for each tuple, we generate a natural-language query using a second prompt.
In the first step, we prompt the LLM like this:
Sample Prompt
Generate 10 random combinations of (feature, client persona, scenario) for a real estate CRM assistant.

The dimensions are:
Feature: what task the agent wants to perform. Possible values: property search, market analysis, scheduling, email drafting.

Client persona: the type of client the agent is working with. Possible values: first-time homebuyer, investor, luxury buyer.

Scenario: how well-formed or challenging the query is. Possible values:
- exact match (clearly specified and feasible),
- ambiguous request (unclear or underspecified),
- shouldn't be handled (invalid or out-of-scope).

Output each tuple in the format: (feature, client persona, scenario)

Avoid duplicates. Vary values across dimensions. The goal is to create a diverse set of queries for our assistant.
For each LLM-generated tuple, we then generate a full query in natural language:
Sample Prompt
We are generating synthetic user queries for a real estate CRM assistant. The assistant helps agents manage client requests by searching listings, analyzing markets, drafting emails, and reading calendars.

Given:
Feature: Scheduling Client Persona: First-time homebuyer
Scenario: Ambiguous request

Write a realistic query that an agent might enter into the system to fulfill this client's request. The query should reflect the client's needs and the ambiguity of the scenario.

Example:
"Find showings for affordable homes with short notice availability."

Now generate a new query.
We continue sampling, generating, and filtering until we reach 100 high-quality, diverse examples.
When phrasing is awkward or content is off-target, we discard and regenerate.
It is better to be aggressive in quality control here: downstream evaluation is entirely based on the representativeness and realism of these traces.
2. Open Coding: Read and Label Traces
With a data set of queries in hand, the next step is to run the assistant on all queries and collect complete traces.
A trace records the entire sequence of steps taken by the pipeline: the initial user query, every LLM output, each downstream tool invocation, and the final user-facing results.
We collect all intermediate and final steps, not just the surface output. Failures often arise inside the chain, not only at the end.
This process is adapted from grounded theory, a methodology from qualitative research that builds theories and taxonomies directly from data.
Rather than starting with a fixed list of error types, we observe how the system behaves, label interesting or problematic patterns, and let the structure of failures emerge naturally.
In grounded theory, coding refers to assigning short descriptive labels to parts of the data that seem important.
For each trace, we read carefully and write brief notes about what we observe: where outputs are incorrect, where actions are surprising, or where the behavior feels wrong or unexpected.
When beginning, we recommend examining the entire trace as a whole and noting the first (most upstream) failure observed.
We record each trace and its corresponding open-coded notes into a simple table or spreadsheet:
At this stage, we do not attempt to group or formalize errors. We simply collect a rich set of raw observations.
Sometimes, when starting with initial labeling, it can be challenging to articulate precisely what feels "off" about a trace.
If we find ourselves stuck, a helpful strategy is to switch temporarily to a more "top-down" approach.
Consider a list of common LLM failure categories and actively look for their manifestations: hallucination, structured output issues, adherence to constraints, stylistic consistency.
Continue this initial annotation until we have surfaced a sufficiently broad set of failures.
As a rule of thumb, we proceed until at least 20 bad traces are labeled and no fundamentally new failure modes are appearing.
This point is known as theoretical saturation: when additional traces reveal few or no new kinds of errors.
3. Axial Coding: Structuring and Merging Failure Modes
Open labeling produces a valuable but chaotic collection of observations.
Without further organization, we cannot meaningfully quantify failures.
Axial coding means reading through the body of open-coded traces and clustering similar failure notes together.
Some patterns are obvious. Traces where the assistant proposes showings for weekends when the real estate agent is marked unavailable cluster naturally into a broader failure mode: violation of user constraints.
Other failures reveal deeper distinctions only after reading several traces.
Hallucinations of property featuresâ€”claiming a home has solar panels when it does notâ€”and hallucinations of client activityâ€”scheduling a tour that the user never requestedâ€”were initially grouped together.
But over time, it becomes clear they differ meaningfully: one misleads about external facts; the other fabricates user intent.
The goal is to define a small, coherent, non-overlapping set of binary failure types, each easy to recognize and apply consistently during trace annotation.
You can use a language model to assist the process. After open coding 30â€“50 traces, we can paste the raw failure notes into our favorite LLM (e.g., ChatGPT, Claude) and ask it to propose preliminary groupings:
Sample Prompt
Below is a list of open-ended annotations describing failures in an LLM-driven real estate CRM assistant. Please group them into a small set of coherent failure categories, where each category captures similar types of mistakes. Each group should have a short descriptive title and a brief one-line definition. Do not invent new failure types; only cluster based on what is present in the notes.
LLM-generated groupings can help organize initial ideas, but they should not be accepted blindly.
Proposed clusters often require manual review and adjustment to accurately reflect the system's behavior and the application's specific requirements.
4. Labeling Traces with Structured Failure Modes
At this stage, we have two artifacts:
A collection of traces, each with its initial, freeform "first-pass annotations."
A defined list of structured, binary failure modes (from axial coding)
Our next goal is to systematically apply these structured failure modes to each trace.
For every trace, and for each defined failure mode, we determine if that specific failure is present (1) or absent (0).
Review and Map Open Codes to Structured Failures: We revisit our initial spreadsheet of traces and their first-pass annotations.
For each trace, we compare its freeform annotation against our defined list of structured failure modes.
Populating the Structured Data Table: We augment our spreadsheet by adding new columns, one for each structured failure mode.
For each trace, we then populate these columns with a 1 if the failure mode is present for that trace and a 0 if it's absent.
For example, if a first-pass annotation for a trace was "SQL query missed the budget constraint and also used an aggressive tone in the generated email," and our structured failure modes include "Missing SQL Constraint" and "Inappropriate Tone," this trace would get a 1 in both those columns and 0s in others.
During this process, it is common to discover inconsistencies or edge cases that force a reevaluation.
We allow ourselves to adjust annotations or revise failure mode definitions as needed.
Once labeling is complete, we can quantify the prevalence of each failure modeâ€”which is critical for prioritization in the Improve phase.
Common Pitfalls
The most common mistake in early error analysis is failing to test on representative data.
If the initial query set does not reflect the diversity and difficulty of real user behavior, the traces produced are uninformative.
A second common failure is skipping open coding altogether.
Instead of reading real traces and observing how the system actually fails, teams often default to generic categories pulled from LLM research: "hallucination," "staying on task," "verbosity."
The abuse of generic metrics is endemic in the industry as many eval vendors promote off the shelf metrics, which ensnare unsuspecting engineers into superfluous metrics and tasks.
Another frequent pitfall is the inappropriate use of Likert scales during early annotation.
In contrast, forcing binary decisions about specific failure modesâ€”whether a problem occurred or notâ€”produces more reproducible annotations.
Finally, treating initial annotations and failure modes as fixed is a critical error.
It is normal for annotation schemas to evolve after reviewing more data. Freezing the schema too early locks evaluation infrastructure around an incomplete understanding of system behavior.
The Analyze phase is the cornerstone of effective LLM evaluation, providing the deep, qualitative understanding necessary before any meaningful measurement or improvement can occur.
Its critical output is a well-understood, application-specific vocabulary of failure: a clear, consistent set of defined failure modes that allows us to precisely describe, and subsequently measure, how and why our LLM pipeline isn't meeting expectations.
This foundation is essential before proceeding to measure these failures at scale.
Next up: Step 2: Measure - implementing automated evaluators to translate qualitative insights into quantitative metrics.
The Layer Deeper: Step 2 - Measure
In Section 3, we presented a framework to identify failures as part of the "Analyze" phase of our evaluation lifecycle. 
Here, we present how to approach the Measure phase.
Measurement is about estimating the prevalence of our application's failure modes.
The ability to quickly and reliably measure changes in success or failure rates is fundamental to an effective development loop. 
When we modify a prompt, adjust a retrieval strategy, or swap a model, we need to see concrete evidence of whether that change helped, harmed, or had no effect on specific issues.
Manually re-evaluating a large set of traces after every pipeline change is slow and prone to inconsistency. 
Therefore, this section focuses on building automated evaluators.
Automated evaluators can compute various types of metrics. 
Some metrics will be reference-free, assessing inherent qualities of an output or its adherence to certain rules without needing a "golden" or ground-truth answer.
Others will be reference-based, comparing the pipeline's output to a known correct or ideal response.
For many failure modes, it is beneficial to conceptualize and, where feasible, implement both reference-free and reference-based checks.
We continue with the same running example from Section 3: a real estate chat-based CRM assistant that interprets natural language queries from real estate agents, issues SQL, drafts client emails, reads calendars, and performs internet searches.
Defining the Right Metrics (What to Measure)
Effective measurement begins by defining precise, quantifiable metrics for each failure mode identified during our error analysis.
To illustrate, the following table lists several categories of failures we might have uncovered for our real estate CRM assistant during error analysis.
After listing the failure modes, we can identify which ones can be simple to fix, due to ambiguity in our prompt specifications.
Gulf of Specification errors, stemming from ambiguous or incomplete instructions we provided to the LLM, can be easy to resolve if we edit the prompt. The evals that we write should mainly target the Gulf of Generalization.
In other words, we want to fix ambiguity first, then measure generalization.
There are two reasons for this. 
First, for efficiency reasons: many specification failures can be resolved rapidly, often by simply adding clarity or detail to an existing prompt. It can feel like a waste of time to build an automated evaluator for a failure mode that is easily resolved by a modification to a prompt.
Second, more fundamentally, we want our evaluation efforts to accurately reflect the LLM's ability to generalize correctly from clear instructions, not its capacity to decipher our potentially ambiguous intents.
Consider an example from Table 2: "Incomplete Email Content" might be a Specification Failure if our email generation prompt doesn't clearly instruct the LLM on mandatory fields to include based on the context. We would need to improve our prompt. In contrast:
"Missing SQL Constraints," if the prompt clearly stated the constraint (e.g., "max_price: $600k") but the LLM still omitted it, points to a Generalization Failure. This is a good candidate for an automated evaluator.
"Persona-Tone Mismatch," assuming the client persona was clearly provided to the LLM, would also be a Generalization Failure.
"Invalid Tool Usage" (calling a non-existent tool) is almost always a Generalization Failure (or a more fundamental model issue) if the available tools are well-defined to the LLM.
Once we have this refined list of Generalization Failures, we design specific, automatable metrics for each.
We try to design both a reference-based and a reference-free metric for each significant failure mode.
For reference-based metrics, first, we need to curate input-output pairs where the LLM output exemplifies the desired failing behavior. The automated evaluator will then measure how closely the pipeline's actual output aligns with this "golden" reference.
For example, if a failure mode is "Missing SQL Constraints," our reference data would consist of user queries and the corresponding ideal SQL, which does include all necessary constraints.
Reference-based metrics are invaluable during iterative development, such as prompt engineering or model fine-tuning, and are often key components of Continuous Integration (CI) checks.
For reference-free metrics, we define intrinsic properties or rules related to the failure mode. The evaluator measures adherence to these defined properties.
For instance, continuing with "Missing SQL Constraints," a reference-free check might involve parsing the generated SQL to verify the presence of keywords associated with expected constraints.
For "Persona-Tone Mismatch," a reference-free approach might involve using an external judgment process to classify the tone of the generated email and then check if this classification aligns with the target persona's known preferences.
Reference-free metrics are particularly powerful because they can often be adapted to run efficiently at scale on new, unlabeled data, enabling broader monitoring and assessment, potentially even in online settings.
As part of both types of metrics, we might incorporate executability checks. This means the metric goes beyond static analysis of the generated text to assess its functional correctness.
For instance, if the LLM generates a SQL query, the evaluator might execute it against a test database to ensure it runs without error and yields plausible results.
The following Table presents sample reference-based and reference-free metrics for some of the failure modes identified in Table 2.
Implementing Metrics (How to Measure)
The goal of automated evaluators is to estimate the prevalence of each failure mode across a set of traces.
Manual labeling at this scale is too slow and expensive, so we turn to automation. The challenge is ensuring each evaluator is well-grounded in the human-defined failure criteriaâ€”otherwise, the metrics are meaningless.
Each failure mode should have its own dedicated evaluator.
Depending on the nature of the failure, this can be implemented either with code or with an LLM.
Code-based evaluators are ideal when the failure definition is objective and can be checked with rules. For example:
Parsing structure (e.g., checking JSON validity or SQL syntax).
Using regex or string matching to detect required or forbidden phrases.
Counting or verifying structural constraints (e.g., a summary has 3 bullet points).
Executing the tool call generated by an LLM, and checking that the execution did not raise any errors.
Logical checks (e.g., if the query asks for a dog-friendly apartment, does the output mention "pets allowed"?)
Code evaluators are fast, cheap, deterministic, and interpretable.
They can support both reference-free checks (e.g., length limits) and reference-based ones (e.g., value extraction compared to a gold set).
LLM-as-Judge evaluators are used when the failure mode involves interpretation or nuance that code can't capture.
For instance: Was the tone appropriate? Is the summary faithful? Is the response helpful?
In these cases, we use a separate LLMâ€”distinct from the main applicationâ€”to judge outputs for a single failure mode. Each metric may require a different LLM-as-Judge evaluator, tailored to its specific failure definition.
This is expectedâ€”and desirableâ€”when evaluating multiple aspects of quality. LLM judges work best for narrowly defined, binary tasks (e.g., when the answer can be PASS or FAIL).
However, using LLMs for evaluation comes with risks: bias, inconsistency, and inference cost. Prompt design, clear criteria, and calibration against human judgments are important.
How do you know you can trust the LLM judge? In a later section, we show you how to verify your judge against the human labels you collected in a previous step.
Writing LLM-as-Judge Prompts
The term "LLM-as-a-judge" was introduced by Zheng et al. (2023), who trained specialized LLM judges using large amounts of human preference data.
In practice, however, such fine-tuning is rarely feasible. Most applications lack sufficient labeled preference data, especially in early stages of development.
Instead, we use off-the-shelf LLMs as judges and rely on prompt engineering to align their outputs with our intended failure definitions.
Each evaluator corresponds to a single metric, which targets a specific failure mode identified during error analysis.
To make supervision tractable without fine-tuning, we restrict the task to binary outputsâ€”PASS/FAIL or YES/NO.
A well-structured LLM-as-Judge prompt contains four essential components:
Clear task and evaluation criterion. Each prompt should focus on one well-scoped failure mode. Vague tasks lead to unreliable judgments. Instead of asking whether an email is "good," we ask whether "the tone is appropriate for a luxury buyer persona."
Precise Pass/Fail definitions. We define what counts as a Pass (failure absent) and a Fail (failure present), directly based on the failure descriptions developed during error analysis.
Few-shot examples. Examples help calibrate the judge's decision boundary. We include labeled outputs that clearly Pass and clearly Fail. These are best drawn from human-labeled traces. While we primarily focus on binary (Pass/Fail) judgments for clarity and alignment, if using finer-grained scales (e.g., 1â€“3 severity), it's critical to include examples for every point on the scale.
Structured output format. The judge should respond in a consistent, machine-readable formatâ€”typically a JSON object with two fields: reasoning (1â€“2 sentence explanation) and answer ("Pass" or "Fail"). This structure improves both accuracy and interpretability.
The example below shows a prompt template for evaluating tone appropriateness in a real estate CRM assistant. Each LLM-as-Judge evaluator follows this structure but is tailored to its specific failure mode.
Sample Prompt
You are an expert evaluator assessing outputs from a real estate assistant chatbot.

Your Task: Determine if the assistant-generated email to a client uses a tone appropriate for the specified client persona.

Evaluation Criterion: Tone Appropriateness

Definition of Pass/Fail:
- Fail: The email's tone, language, or level of formality is inconsistent with or unsuitable for the described client persona.
- Pass: The email's tone, language, and formality align well with the client persona's expectations.

Client Personas Overview:
- Luxury Buyers: Expect polished, highly professional, and deferential language. Avoid slang or excessive casualness.
- First-Time Homebuyers: Benefit from a friendly, reassuring, and patient tone. Avoid overly complex jargon.
- Investors: Prefer concise, data-driven, and direct communication. Avoid effusiveness.

Output Format: Return your evaluation as a JSON object with two keys:
1. reasoning: A brief explanation (1-2 sentences) for your decision.
2. answer: Either "Pass" or "Fail".

Examples:
â€“â€“â€“
Input 1:
Client Persona: Luxury Buyer
Generated Email: "Hey there! Got some truly awesome listings for you in the high-end district. Super views, totally posh. Wanna check 'em out ASAP?"

Evaluation 1: {"reasoning": "The email uses excessive slang ('Hey there', 'awesome', 'totally posh', 'ASAP') and an overly casual tone, which is unsuitable for a Luxury Buyer persona.", "answer": "Fail"}

â€“â€“â€“
Input 2:
Client Persona: First-Time Homebuyer
Generated Email: "Good morning! I've found a few properties that seem like a great fit for getting started in the market, keeping your budget in mind. They offer good value and are in nice, welcoming neighborhoods. Would you be interested in learning more or perhaps scheduling a visit?"

Evaluation 2: {"reasoning": "The email adopts a friendly, reassuring tone ('great fit for getting started', 'nice, welcoming neighborhoods') suitable for a First-Time Homebuyer, and clearly offers next steps.", "answer": "Pass"}

â€“â€“â€“
Now, evaluate the following:
Client Persona: {{CLIENT_PERSONA_HERE}}
Generated Email: {{GENERATED_EMAIL_HERE}}

Your JSON Evaluation:
Data Splits for Designing and Validating LLM-as-Judge
Designing an LLM-as-Judge closely resembles training a classifierâ€”except the "training" happens through prompt engineering, not parameter tuning.
Instead of learning from data via gradient descent, we manually select examples and write instructions that guide the model's behavior in-context.
To ensure the resulting evaluator generalizes and doesn't overfit, we divide our labeled traces (from Section 3) into three disjoint sets:
Training Set. A pool of labeled examples we may draw from when constructing the prompt. These examples are candidates for few-shot demonstrationsâ€”typically clear-cut Pass and Fail cases that illustrate the boundaries of the failure mode.
Development (Dev) Set. A separate, larger set of labeled traces used to refine the prompt. After each editâ€”whether changing instructions, rewording criteria, or swapping few-shot examplesâ€”we evaluate judge outputs on the dev set by comparing them to human labels. Dev set examples must never appear in the prompt itself. This ensures we can measure how well the judge generalizes beyond the training examples.
Test Set. A held-out set we use only to compute the alignment of the LLM judge, after finalizing the LLM judge prompt. We never look at this set during judge prompt development. It gives us an unbiased estimate of the judge's real-world accuracyâ€”metrics like True Positive Rate and True Negative Rateâ€”which we later use to adjust estimated success rates in production traces.
Reusing examples across splitsâ€”especially from Dev or Test in the promptâ€”leads to overfitting and inflated accuracy estimates.
Unlike traditional supervised learning, we don't need large training sets. In-context learning typically saturates after a small number of well-chosen examples. Studies suggest that LLM performance often plateaus after 1â€“8 examples. More examples can even degrade accuracy or exceed context length limits.
As a result, we allocate more data to evaluation. A typical split might assign 10-20% of labeled traces to the training set, and 40-45% each to dev and test.
To reliably estimate judge performance, both dev and test sets should include a balanced number of Pass and Fail examplesâ€”ideally 30â€“50 of each.
These proportions may not reflect real-world prevalence, especially when failures are rare. This imbalance is acceptable and deliberate.
Iterative Prompt Refinement for the LLM-as-Judge
After splitting labeled data into training, development, and test sets, we enter the core of judge construction: iteratively refining the prompt to align the LLM's decisions with expert labels.
This process mirrors the tuning loop of a classifier, but instead of adjusting parameters, we revise prompt text and examples. The loop proceeds as follows:
Write a Baseline Prompt. Start with an initial prompt using the components outlined earlier: task description, clear definitions, structured output format, and a few-shot example set drawn from the training set.
Evaluate on dev set. Run the LLM-as-Judge over all examples in the development set. Compare each judgment (Pass or Fail) to the human-provided ground truth.
Measure agreement. Let P be the total number of dev examples labeled Pass, of which p were judged Pass; let F be the total number labeled Fail, of which f were judged Fail. Then compute:
TPR = p/P (true positive rate), TNR = f/F (true negative rate).
We refer to a positive as a pass, and a negative as a fail.
Inspect disagreements. Review false passes (judge said Pass but human said Fail) and false fails (judge said Fail but human said Pass) to identify ambiguous criteria or missing edge cases.
Refine the Prompt. Based on dev set errors:
Clarify task wording or tighten Pass/Fail criteria.
Swap in more illustrative few-shot examples from the training set.
If the dev set has some failure modes that are not in the training set, add representative traces to the training set and consider using them in the prompt for the next round.
Repeat. Re-evaluate the revised prompt on the same dev set and re-calculate metrics. Continue until performance stabilizes or improves acceptably.
When to Stop Refining
We stop when TPR and TNR reach satisfactory levels (e.g., >90%). Thresholds depend on application needsâ€”missing a real failure may be costlier than flagging a false one.
While tools like DSPy can automate this loop by optimizing prompts over a dev set, we recommend manual iteration first. It builds intuition about both the failure mode and the judge's behavior.
If Alignment Stalls
If the judge continues to perform poorlyâ€”e.g., low TPR and TNRâ€”consider one of the following strategies:
We could use a more capable LLM: a larger or newer model may resolve subtle or context-sensitive errors.
We could also decompose the criterion, or break a complex failure mode into smaller, more atomic checks, each with its own judge.
Or, we could improve our labeled data, or add more diverse, high-quality examples to the training setâ€”especially for edge cases.
It is important to make sure that we trust our labels. A common mistake companies make is to outsource labeling to people (or even LLMs) who do not have any context on the application.
Estimating True Success Rates with Imperfect Judges
After achieving consistent, high TPR and TNR on the dev set, we fix the LLM-as-Judge prompt and run it on the test set.
This gives us two things. First, an estimated pass or success rate on the test set: the fraction of examples the judge labels as "Pass." Second, estimates of the judge's TPR and TNR, computed by comparing its predictions to ground-truth human labels.
But this only tells us how the judge behaves on the test split. In practice, we want to estimate how often a failure mode appears in a much larger (often unlabeled) datasetâ€”such as new traces from a production pipeline.
The problem is that our judge is imperfect. If we run it over thousands of new outputs and simply count the "Pass" predictions, we'll get a biased estimate of the true pass rate, because the judge occasionally misses failures or flags passes incorrectly.
This section shows how to:
Use the judge's TPR and TNR to correct its raw predictions and estimate the true pass rate Î¸ for a metricâ€”that is, the fraction of examples a human would consider passing the evaluation criterion.
Quantify the uncertainty around this estimate by constructing a 95% confidence interval. If the upper bound is low, we can trust the LLM pipeline is performing acceptably. If the interval is wide, we may need to improve the TPR and TNR of the judge.
We now present a procedure to estimate the true pass or success rate Î¸ of an evaluation metric over new, unlabeled traces.
Step 1: Measure Judge Accuracy. On our held-out test set, we compare judge predictions to human labels and compute TPR and TNR.
Step 2: Observe Raw Success Rate. We run the judge on m new, unlabeled traces and let k be the number it labels "Pass." The raw success rate is p_obs = k/m.
Step 3: Correct the Observed Success Rate. Because the judge is imperfect, p_obs is biased. We adjust it to estimate the true success rate:
Î¸Ì‚ = (p_obs + TNR - 1) / (TPR + TNR - 1) (clipped to [0, 1]).
If TPR + TNR - 1 â‰ˆ 0 (e.g., 50% TPR and TNR), then the judge is no better than random chance, and the correction is invalid. In practice, Î¸Ì‚ is clipped to the range [0, 1] to handle numerical noise.
Step 4: Quantify Uncertainty with Bootstrap Sampling. We quantify uncertainty in our corrected success rate estimate by bootstrapping over the test set's judge-vs-human labels: each iteration samples (with replacement) the full set of (human label, judge prediction) pairs, recomputes TPR and TNR on that sample, applies the correction to obtain a new Î¸Ì‚*, and records it. After B iterations, the 2.5th and 97.5th percentiles of these {Î¸Ì‚*} values form our 95% confidence interval.
Here is a Python code snippet that computes a point estimate of the LLM pipeline's success rate Î¸Ì‚, as well as a 95% confidence interval for the true success rate Î¸.
import numpy as np

def estimate_success_rate(
    test_labels,
    test_preds,
    unlabeled_preds,
    B=20000
):
    """
    Args:
        test_labels: array-like of 0/1, human labels on test set (1 = Pass).
        test_preds: array-like of 0/1, judge predictions on test set (1 = Pass).
        unlabeled_preds: array-like of 0/1, judge predictions on unlabeled data (1 = Pass).
        B: number of bootstrap iterations.

    Returns:
        theta_hat: point estimate of true success rate.
        L, U: lower and upper bounds of a 95% bootstrap CI.
    """
    test_labels = np.asarray(test_labels, dtype=int)
    test_preds = np.asarray(test_preds, dtype=int)
    unlabeled_preds = np.asarray(unlabeled_preds, dtype=int)

    # Step 1: Judge accuracy on test set
    P = test_labels.sum()
    F = len(test_labels) - P
    TPR = ((test_labels == 1) & (test_preds == 1)).sum() / P
    TNR = ((test_labels == 0) & (test_preds == 0)).sum() / F

    # Step 2: Raw observed success rate
    p_obs = unlabeled_preds.sum() / len(unlabeled_preds)

    # Step 3: Correct estimate
    denom = TPR + TNR - 1
    if denom <= 0:
        raise ValueError("Judge accuracy too low for correction")
    theta_hat = (p_obs + TNR - 1) / denom
    theta_hat = np.clip(theta_hat, 0, 1)

    # Step 4: Bootstrap CI
    N = len(test_labels)
    idx = np.arange(N)
    samples = []
    for _ in range(B):
        boot_idx = np.random.choice(idx, size=N, replace=True)
        lbl_boot = test_labels[boot_idx]
        pred_boot = test_preds[boot_idx]
        P_boot = lbl_boot.sum()
        F_boot = N - P_boot
        if P_boot == 0 or F_boot == 0:
            continue
        TPR_star = ((lbl_boot == 1) & (pred_boot == 1)).sum() / P_boot
        TNR_star = ((lbl_boot == 0) & (pred_boot == 0)).sum() / F_boot
        denom_star = TPR_star + TNR_star - 1
        if denom_star <= 0:
            continue
        theta_star = (p_obs + TNR_star - 1) / denom_star
        samples.append(np.clip(theta_star, 0, 1))

    if not samples:
        raise RuntimeError("No valid bootstrap samples; check inputs")

    L, U = np.percentile(samples, [2.5, 97.5])
    return theta_hat, L, U
To build intuition for how the success rate estimate varies with different LLM-as-Judge TPR and TNR, we present a small synthetic experiment. We fix the "ground truth" success rate at 80% and assume 50 human-labeled successes and 50 human-labeled failures.
In the left plot, we assume perfect failure classification (i.e., TNR = 100%) and vary the judge's success detection (i.e., TPR) from 50% to 100%. When detection is only 50%, the uncorrected success rate drops and its 95% CI is extremely wide. As the judge improves, the red curve climbs toward 80%, the CI contracts, and the bias-corrected estimate (blue) remains close to the true value.
In the right plot, we hold true success detection (i.e., TPR) at 100% and vary TNR from 50% to 100%. At 50%, half of real failures slip through as successesâ€”driving the uncorrected rate toward 100% and widening uncertainty. Better TNR corrects this: the red curve falls back to 80%, its confidence interval tightens, and throughout, the bias-corrected line again tracks the true 80%.
Improving the TPRâ€”the judge's ability to correctly identify true successesâ€”tends to narrow the confidence interval for our estimated true success rate (Î¸Ì‚) the most.
Key Takeaway: LLM-as-Judge Alignment Priorities
The correction reliably removes bias in both extreme error modes.
Judge errors mainly inflate uncertainty (wider confidence intervals), rather than shifting the corrected estimate.
Improving TPR, or the judge's ability to identify true successes, narrows the confidence interval the most.
Common Pitfalls
The most common mistake when implementing LLM-as-Judge evaluators is omitting examples from the prompt entirely.
Without concrete examples, the model lacks grounding in what constitutes a failure for the task at hand. This often leads to vague or inconsistent behavior, even when the task definition is otherwise clear.
A second pitfall is attempting to do too much in a single prompt.
Some teams try to evaluate multiple criteria at onceâ€”tone, content correctness, next stepsâ€”in a single Pass/Fail decision. This introduces ambiguity and makes it harder to diagnose errors. Breaking complex metrics into narrower, more specific metrics and prompts yields better alignment and more reliable judges.
Another common issue is skipping the alignment step altogether.
Teams often assume the LLM-as-Judge will "just work" out of the box. Sometimes, that is trueâ€”especially for simple or broadly familiar metrics like sentiment polarity (i.e., positive or negative). But many evaluation metrics are domain-specific or tied to the unique "vibe" of a product: tone alignment for luxury clients, completeness of a listing summary, justification of a tool call. These require effort to align. Judges don't come pretrained on a product's valuesâ€”we have to teach them.
Prompt refinement and human-labeled validation are essential to ensure our evaluators actually reflects what matters in our pipelines. But aligning a judgeâ€”by refining prompts and validating its agreement with human labelsâ€”is an investment that pays off enormously. An aligned judge can replace expensive human evaluation across thousands of traces, making pipeline-wide monitoring feasible.
A related pitfall is overfitting the LLM-as-Judge prompt to their labeled traces.
This often happens when teams include those same traces as examples and as part of the evaluation set used to measure alignment. This contaminates the metrics: TPR and TNR may appear high, not because the judge is generalizing well, but because it has memorized specific examples. Any trace used in the prompt must be excluded from the evaluation set.
Moreover, even after alignment, many teams fail to revisit the process.
Production data can drift. New failure modes may emerge, LLM updates may shift behavior, and evaluation metrics may evolve. We recommend re-running the alignment process regularly (e.g., weekly): continue labeling a handful of traces, recomputing TPR and TNR, and checking whether confidence intervals remain acceptably tight. If not, retrain the judge.
This is not a one-off taskâ€”it's part of the ongoing lifecycle of LLM-powered applications, much like monitoring in MLOps. When observing new failure modes in production, we can add examples of them to all three splits (train, dev, and test).
Once we've identified failure modes, implementing evaluators allows us to measure them systematically and at scale.
We define precise, failure-mode-specific metrics, prefer code-based checks when possible, and rely on LLM-as-Judge setups for more complex, nuanced, or domain-specific judgments. By aligning our judges through prompt refinement and validation against human labels, we ensure that automated evaluations reflect what we actually care about.
Evaluation is not a one-time setup. Like any robust system, it must be maintained: revisited as data shifts, retrained as definitions evolve, and monitored for continued alignment. This work is essential for any LLM-powered product that aims to be reliable, interpretable, and improvable.
Next up: Step 3: Improve - using our measurement insights to systematically enhance pipeline performance.
The Layer Deeper: Step 3 - Improve
Based on the error analysis and online monitoring insights, we implement changes to enhance the LLM pipeline. Common strategies for pipeline improvement include:
Refine Prompts
This is often the first and most impactful area for improvement. Small changes in wording, structure, or instructions can yield significant differences in output quality. Consider techniques such as:
Adding Clarity and Specificity: If the LLM struggles with ambiguity (like the "West Berkeley" vs. "Berkeley West" example), try providing more explicit instructions on how to interpret such terms or include clarifying examples directly in the prompt.
Using Few-Shot Examples: For specific problematic scenarios, carefully chosen examples of correct input/output pairs embedded within the prompt can effectively guide the model. Of course, we have to ensure these few-shot examples are distinct from our CI golden set to prevent data leakage and maintain the integrity of our regression tests.
Encouraging Step-by-Step Reasoning: For tasks requiring complex logic or multi-step generation, explicitly asking the LLM to "think step by step" or to break down its reasoning process before providing the final answer can improve coherence and accuracy.
Decompose Complex Tasks
If a single LLM call struggles with a complex task, break it down into a sequence of smaller, more manageable sub-tasks. 
Each sub-task can then be handled by a separate, specialized LLM call, potentially with its own tailored prompt, different model settings, or even a different underlying model optimized for that specific step.
For instance, instead of asking the real estate assistant to "find suitable properties for a client based on their preferences and draft an email summarizing these options," we could decompose this into:
(a) An LLM call to analyze the client's unstructured request and extract key preferences (e.g., location, price range, number of bedrooms).
(b) A tool call to a database or API (e.g., query_listings) using the extracted structured criteria.
(c) An LLM call to review the retrieved listings, select the most relevant ones based on softer criteria or nuances, and perhaps summarize their key features.
(d) A final LLM call to draft a personalized email to the client, incorporating the selected properties and summaries.
Adjust Retrieval Augmented Generation (RAG) Strategies
If our pipeline uses RAG and we're encountering issues like hallucinations due to irrelevant retrieved context or outdated information, focus on the retrieval mechanism. 
This might involve tuning the retrieval query formulation, adjusting the chunking strategy of our source documents to create more coherent context blocks, modifying the number of documents retrieved, or implementing a re-ranking step to prioritize the most relevant chunks before they are passed to the LLM.
Improve Tool Schemas and Descriptions
When the LLM misuses tools or generates malformed arguments for tool calls, revisit the tool's definition. 
Clarify tool descriptions to be unambiguous and comprehensive. 
Ensure parameter names are intuitive, and their descriptions clearly define their purpose, expected data types, formats, and any constraints. 
For example, if a search_listings tool's neighborhood parameter is causing issues, its description should be refined to specify how to handle sub-regions, common abbreviations, or known ambiguous names.
Add or Refine Guardrails
Based on new failure modes observed, introduce new guardrails or tighten existing ones. 
These are typically fast, deterministic checks (e.g., regex, schema validation, blocklists) that run before output is finalized. 
Remember to balance the strictness of guardrails against the risk of them blocking valid outputs too frequently (false positives), considering the cost-benefit of each.
Consider Fine-Tuning (Judiciously)
If prompt engineering, task decomposition, RAG adjustments, and tool refinements haven't sufficiently addressed a persistent and well-defined failure mode, and we have a substantial dataset of high-quality examples demonstrating the desired behavior, fine-tuning a smaller, open-source model or a proprietary model (if the provider offers such services) might be an option.
 This is generally a more resource-intensive approach and should be considered when other methods prove inadequate for reaching our quality targets for specific, repeatable patterns of failure.
The Continuous Improvement Flywheel
The practices of CI and CD do not exist in isolation. They form components of a continuous improvement cycleâ€”i.e., a "flywheel"â€”that drives the iterative refinement of LLM applications. 
Understanding this cycle helps teams structure their workflow for sustained quality enhancement.
The flywheel can proceed as follows:
Develop & Analyze: Begin with initial pipeline development (e.g., prompts, logic, tool integration). Conduct thorough Error Analysis (Section 3) on early outputs or bootstrapped data to identify key failure modes.
Measure & Build Evals: Translate qualitative failure modes into quantitative metrics. Implement automated evaluators (code-based and LLM-as-Judge) and validate them (Section 4). Use collaborative methods (Section 5) if needed to refine subjective rubrics and build a high-quality golden dataset.
CI Setup: Integrate the golden dataset and automated evaluators into the CI pipeline. Configure automated checks to prevent regressions for known issues.
Deploy (CD): Ship the LLM pipeline with online monitoring components, including observability hooks and configurations for running evaluators on sampled production traffic. Ensure judge models are pinned.
Monitor Online: Actively track corrected failure rates (Î¸) with confidence intervals for key metrics using production data via the observability platform. Set up dashboards and alerts for anomalies or threshold breaches.
Identify Drift / New Failures: Analyze online monitoring data, user feedback, and flagged traces (i.e., those failing online evaluation). Look for significant increases in known failure rates (drift) or new, previously unseen problems.
Example (Real Estate Agent Assistant): Monitoring reveals a spike in the "Location Ambiguity" failure rate after a new neighborhood targeting feature is deployed. Manual review shows the LLM frequently confuses "West Berkeley" with "Berkeley West" (a non-existent area).
Re-Analyze: Perform targeted Error Analysis (Section 3) on the newly identified issues. For example, confirm that the LLM struggles with directional qualifiersâ€”terms like "West," "North," or "Upper" used to specify parts of a location (e.g., "West Berkeley")â€”introduced by the new feature's prompt.
Update Artifacts:
Add examples of the new failure (e.g., "West Berkeley" confusion) to the golden dataset.
Refine or add evaluators to detect the failure mode. Update rubrics and re-validate LLM judges (Section 4.3).
Update CI checks to test for the new failure using the augmented golden set.
Periodically re-run the judge alignment loop (Section 8.3).
Improve Pipeline: Based on the error analysis and online monitoring insights, implement changes to enhance the LLM pipeline.
Example (Continuing Real Estate Assistant): After observing the "West Berkeley" confusion, the team first attempts to improve the main prompt with clearer instructions for handling directional qualifiers in location names and adds a negative example (an example of what not to do). They also add "West Berkeley" vs. "Berkeley West" to their golden dataset to ensure CI catches regressions. If this prompt change isn't sufficient, they might consider decomposing the location understanding part into a separate LLM call that specifically resolves location entities before the main property search.
Re-deploy: Ship the improved pipeline and updated evaluation artifacts. Resume monitoring (Step 5) and expect failure rates to decrease.
This flywheel emphasizes that evaluation is not a one-time phase but an ongoing, iterative process. Discoveries during online monitoring feed back into error analysis, refinement of evaluation artifacts, and ultimately, pipeline improvements.
Cycling through Analyze, Measure, and Improve (Figure 2) creates a powerful feedback loop. It uses structured evaluation to systematically navigate the complexities posed by the Three Gulfs, leading to more reliable and effective LLM applications.
This completes our deep dive into the Analyze-Measure-Improve evaluation lifecycle. Building reliable, high-quality LLM applications demands treating evaluation as an ongoing, integrated engineering discipline, not a one-off task.
This was but a taste of whatâ€™s covered in Hamelâ€™s excellent course, AI Evals for Engineers & PMs course [ https://substack.com/redirect/70ae8b70-ff4a-484c-8bb4-f47027acefec?j=eyJ1IjoiYXpucmUifQ.-8uBcpk8-HDATmkL0LtRV3v-RBLbqZCsv5EbXeOlA6M ]. 
If you want to go further, use my code ag-product-growth [ https://substack.com/redirect/70ae8b70-ff4a-484c-8bb4-f47027acefec?j=eyJ1IjoiYXpucmUifQ.-8uBcpk8-HDATmkL0LtRV3v-RBLbqZCsv5EbXeOlA6M ] to get over $800 off the course. 
Weâ€™re also recording a podcast soon. What questions should we cover there? Sound off in the comments.
Yes, you can reply to this email with more detailed questions, and weâ€™ll answer them!
P.S. Consider asking your employer to expense [ https://substack.com/redirect/efd600b0-40bd-4562-9f27-7d333cd5d3bd?j=eyJ1IjoiYXpucmUifQ.-8uBcpk8-HDATmkL0LtRV3v-RBLbqZCsv5EbXeOlA6M ] an annual subscription.
Additional AI PM Content
If youâ€™re on the journey to becoming a better AI PM, youâ€™ll love these:
Newsletter - AI PM Skills
Your Guide to AI Product Strategy [ https://substack.com/redirect/b9637b2c-8a07-456e-a163-8085423107ca?j=eyJ1IjoiYXpucmUifQ.-8uBcpk8-HDATmkL0LtRV3v-RBLbqZCsv5EbXeOlA6M ]
Ultimate Guide to AI Prototyping Tools (Lovable, Bolt, Replit, v0) [ https://substack.com/redirect/51a6b74b-230d-4319-89ff-6ff1048ec63b?j=eyJ1IjoiYXpucmUifQ.-8uBcpk8-HDATmkL0LtRV3v-RBLbqZCsv5EbXeOlA6M ]
Most People are Building AI Products Wrong - Here's How to do it Right [ https://substack.com/redirect/d3b8946b-7f37-413d-8da3-714da95ab776?j=eyJ1IjoiYXpucmUifQ.-8uBcpk8-HDATmkL0LtRV3v-RBLbqZCsv5EbXeOlA6M ]
The AI PM's Playbook: How Top Product Managers Are 10x-ing Their Impact [ https://substack.com/redirect/8a68bc10-67be-49c3-85b9-83153583e0d6?j=eyJ1IjoiYXpucmUifQ.-8uBcpk8-HDATmkL0LtRV3v-RBLbqZCsv5EbXeOlA6M ]
Newsletter - Becoming an AI PM
How to Become an AI PM [ http://How to become an AI Product Manager ]
How to Land Your Dream Job at OpenAI [ https://substack.com/redirect/de7ea992-d984-4145-8cdf-82d995cfac3b?j=eyJ1IjoiYXpucmUifQ.-8uBcpk8-HDATmkL0LtRV3v-RBLbqZCsv5EbXeOlA6M ]
How to Become an AI Product Manager with No Experience [ https://substack.com/redirect/3ef97852-dc55-4ba7-ae3d-9f41af731d65?j=eyJ1IjoiYXpucmUifQ.-8uBcpk8-HDATmkL0LtRV3v-RBLbqZCsv5EbXeOlA6M ]
How to Write a Killer AI Product Manager Resume (With Examples) [ https://substack.com/redirect/b2db45f1-f84f-453b-aaa6-d2a4610c0eb1?j=eyJ1IjoiYXpucmUifQ.-8uBcpk8-HDATmkL0LtRV3v-RBLbqZCsv5EbXeOlA6M ]
Podcast - AI PM Skills
Tutorial of Top 5 AI Prototyping Tools [ https://substack.com/redirect/06f625b0-7565-42c5-a99a-c48a4bed35fc?j=eyJ1IjoiYXpucmUifQ.-8uBcpk8-HDATmkL0LtRV3v-RBLbqZCsv5EbXeOlA6M ]
Complete Course: AI Product Management [ https://substack.com/redirect/58fa094f-799a-4f5b-9299-5979ebb544cf?j=eyJ1IjoiYXpucmUifQ.-8uBcpk8-HDATmkL0LtRV3v-RBLbqZCsv5EbXeOlA6M ]
We Built an AI Agent to Automate PM in 73 mins (ZERO CODING) [ https://substack.com/redirect/0522645a-0568-4252-86fb-27d4fc9a7299?j=eyJ1IjoiYXpucmUifQ.-8uBcpk8-HDATmkL0LtRV3v-RBLbqZCsv5EbXeOlA6M ]
15 Steps to Build a $100M+ AI Company (From Someone Who Has Done It) [ https://substack.com/redirect/331a7fd8-ccc2-4584-a8db-67bb3cb28425?j=eyJ1IjoiYXpucmUifQ.-8uBcpk8-HDATmkL0LtRV3v-RBLbqZCsv5EbXeOlA6M ]
We Built an AI Product Manager in 58 mins (Claude, ChatGPT, Loom, Notion AI) [ https://substack.com/redirect/4fe2121c-3a38-46cb-8eb5-6f250cde03d7?j=eyJ1IjoiYXpucmUifQ.-8uBcpk8-HDATmkL0LtRV3v-RBLbqZCsv5EbXeOlA6M ]

Unsubscribe https://substack.com/redirect/2/eyJlIjoiaHR0cHM6Ly93d3cubmV3cy5hYWthc2hnLmNvbS9hY3Rpb24vZGlzYWJsZV9lbWFpbD90b2tlbj1leUoxYzJWeVgybGtJam94T0RRMU9Ua3hOQ3dpY0c5emRGOXBaQ0k2TVRZME9EVTJNak00TENKcFlYUWlPakUzTkRreU5USXdNellzSW1WNGNDSTZNVGM0TURjNE9EQXpOaXdpYVhOeklqb2ljSFZpTFRRMU5EQXdNeUlzSW5OMVlpSTZJbVJwYzJGaWJHVmZaVzFoYVd3aWZRLjEwTFV3VWEyc2RkX0Q3RGY2MG1ueDFHZkk5d0dBVF9lR1NzVGtYLUJpOXciLCJwIjoxNjQ4NTYyMzgsInMiOjQ1NDAwMywiZiI6ZmFsc2UsInUiOjE4NDU5OTE0LCJpYXQiOjE3NDkyNTIwMzYsImV4cCI6MTc1MTg0NDAzNiwiaXNzIjoicHViLTAiLCJzdWIiOiJsaW5rLXJlZGlyZWN0In0.SWKgKifrO6_3rPQPs6b4WE_0CvFKWFSN8riPx4tmPoA?